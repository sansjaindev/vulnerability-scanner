import aiohttp
import requests
from urllib.parse import urljoin, urlparse
from form_functions import get_form_details, get_forms
import logging
from logger import set_logger
from bs4 import BeautifulSoup as bs


set_logger()
log = logging.getLogger(__name__)

s = requests.Session()
s.headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"


def is_vulnerable(response):
	"""
	A simple boolean function that determines whether a page 
	is SQL Injection vulnerable from its `response`
	"""
	errors = [
		"you have an error in your sql syntax;",
		"warning: mysql",
		"syntax error",
		# SQL Server
		"unclosed quotation mark after the character string",
		# Oracle
		"quoted string not properly terminated",
	]

	for error in errors:
		if error in response.content.decode().lower():
			return True
	
	return False


def sqli(url):
	"""
	Given a `url`, it returns the link of sqli vulnerable locations if any for the webpage.
	"""

	log.info(f'Recieved sql query on {url}')

	result = None
	
	for c in "\"'":
		new_url = f"{url}{c}"
		log.info(f'[!] Trying {new_url}')
		
		try:
			res = s.get(new_url)
			if is_vulnerable(res):
				log.info(f'[+] SQL Injection vulnerability detected, link: {new_url}')
				result = f"""[+] SQL Injection vulnerability detected, link: {new_url}"""
				return result
		
		except:
			continue
	
	log.info(f'[-] SQL Injection Vulnerability not detected in URL : {url}')
	forms = get_forms(url)
	if len(forms) == 0:
		exit()

	log.info('[+] Trying SQL Injection in internal forms')
	log.info(f'[+] Detected {len(forms)} forms on {url}')
	result = f'[+] Detected {len(forms)} forms on {url}\n'

	for form in forms:
		form_details = get_form_details(form)

		for c in "\"'":
			data = {}

			for input in form_details['inputs']:
				if (input['type'] != 'submit' and input['type'] != 'password'):
					data[input['name']] = f"test{c}"
				
				if input['type'] == 'password':
					data[input['name']] = 'test'

			url = urljoin(url, form_details['action'])

			if form_details['method'] == 'post':
				response = s.post(url, data=data)
			
			if form_details['method'] == 'get':
				response = s.get(url, params=data)

			if is_vulnerable(response):
				result += f"""\n[+] SQL Injection vulnerability detected, link: {url}\n"""
				log.info(f'[+] SQL Injection vulnerability detected, link: {url}')
				return result
			
	result += f"""\n[-] No SQL Injection vulnerability detected, link: {url}\n"""
	log.info(f'[-] No SQL Injection vulnerability detected, link: {url}')
	return result


def submit_form(form_details, url, value):
	"""
	Submits a form given in `form_details`
	Returns the HTTP Response after form submission
	"""

	target_url = urljoin(url, form_details["action"])
	inputs = form_details["inputs"]
	data = {}

	for input in inputs:
		if input["type"] == "text" or input["type"] == "search":
			input["value"] = value
		
		input_name = input.get("name")
		input_value = input.get("value")
		
		if input_name and input_value:
			data[input_name] = input_value

	log.info(f"[!] Submitting malicious payload to {target_url}")
	log.info(f"[+] Data: {data}")
	print(f"[!] Submitting malicious payload to {target_url}")
	print(f"[+] Data: {data}")

	if form_details["method"] == "post":
		return requests.post(target_url, data=data)
	
	else:
		return requests.get(target_url, params=data)


def xss(url):
	"""
	Given a `url`, it prints all XSS vulnerable forms and 
	returns True if any is vulnerable, False otherwise
	"""

	found = False

	log.info(f'[!] Trying XSS on {url}')

	result = None
	forms = get_forms(url)
	log.info(f"[+] Detected {len(forms)} forms on {url}")
	print(f"[+] Detected {len(forms)} forms on {url}")
	
	js_script = "<script>alert('alert')</script>"
	
	for form in forms:
		form_details = get_form_details(form)
		content = submit_form(form_details, url, js_script).content.decode()

		if js_script in content:
			found = True
			

			result = f"[+] XSS Detected on {url}"
			result += f"\n[*] Form details : "
			result += '\n' + str(form_details)
	
	if found:
		log.info(f"[+] XSS Detected on {url}")
	
	else:
		log.info(f"[-] No XSS Vulnerability detected in {url}")
		result = f'[-] No XSS Vulnerability detected in {url}'

	return result



async def fetch(url, session):
	async with session.get(url) as response:
		try:
			return await response.text(encoding='utf-8')
		except UnicodeDecodeError:
			return await response.text(errors='replace')

async def web_crawl(base_url):
	if not base_url.startswith('http://') and not base_url.startswith('https://'):
		base_url = 'http://' + base_url
	base_domain = urlparse(base_url).netloc
	visited_urls = set()
	urls_to_visit = [base_url]

	async with aiohttp.ClientSession() as session:
		async def scrape_links(url):
			nonlocal urls_to_visit
			try:
				html = await fetch(url, session)
				soup = bs(html, "html.parser")

				links = soup.find_all("a", href=True)
				for link in links:
					href = link.get("href")
					if href and href not in visited_urls and href != '#':
						print(f"[+] Found new link: {href}")
						visited_urls.add(href)
						absolute_url = urljoin(base_url, href)
						if urlparse(absolute_url).netloc == base_domain:
							urls_to_visit.append(absolute_url)

				form_links = soup.find_all('form', action=True)
				for link in form_links:
					link = link.get('action')
					if link and link not in visited_urls and link != '#':
						print(f"[+] Found new form link: {link}")
						visited_urls.add(link)
						absolute_url = urljoin(base_url, link)
						if urlparse(absolute_url).netloc == base_domain:
							urls_to_visit.append(absolute_url)

				js_links = soup.find_all('script', src=True)
				for link in js_links:
					link = link.get('src')
					if link and link not in visited_urls and link != '#':
						print(f"[+] Found new script source link: {link}")
						visited_urls.add(link)
						absolute_url = urljoin(base_url, link)
						if urlparse(absolute_url).netloc == base_domain:
							urls_to_visit.append(absolute_url)

				img_links = soup.find_all('img', src=True)
				for link in img_links:
					link = link.get('src')
					if link and link not in visited_urls and link != '#':
						print(f"[+] Found new image path: {link}")
						visited_urls.add(link)
						absolute_url = urljoin(base_url, link)
						if urlparse(absolute_url).netloc == base_domain:
							urls_to_visit.append(absolute_url)

			except aiohttp.ClientConnectionError:
				pass

		async def process_urls():
			while urls_to_visit:
				url = urls_to_visit.pop(0)
				await scrape_links(url)

		await process_urls()
	
	return visited_urls