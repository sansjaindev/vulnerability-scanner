import requests
from urllib.parse import urljoin, urlparse
from form_functions import get_form_details, get_forms
import logging
from logger import set_logger
from bs4 import BeautifulSoup as bs
from queue import Queue
import threading


set_logger()
log = logging.getLogger(__name__)

s = requests.Session()
s.headers["User-Agent"] = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"


def is_vulnerable(response):
	"""
	A simple boolean function that determines whether a page 
	is SQL Injection vulnerable from its `response`
	"""
	errors = [
		"you have an error in your sql syntax;",
		"warning: mysql",
		"syntax error",
		# SQL Server
		"unclosed quotation mark after the character string",
		# Oracle
		"quoted string not properly terminated",
	]

	for error in errors:
		if error in response.content.decode().lower():
			return True
	
	return False


def sqli(url):
	"""
	Given a `url`, it returns the link of sqli vulnerable locations if any for the webpage.
	"""

	log.info(f'Recieved sql query on {url}')

	result = None
	
	for c in "\"'":
		new_url = f"{url}{c}"
		log.info(f'[!] Trying {new_url}')
		
		try:
			res = s.get(new_url)
			if is_vulnerable(res):
				log.info(f'[+] SQL Injection vulnerability detected, link: {new_url}')
				result = f"""[+] SQL Injection vulnerability detected, link: {new_url}"""
				return result
		
		except:
			continue
	
	log.info(f'[-] SQL Injection Vulnerability not detected in URL : {url}')
	forms = get_forms(url)
	if len(forms) == 0:
		exit()

	log.info('[+] Trying SQL Injection in internal forms')
	log.info(f'[+] Detected {len(forms)} forms on {url}')
	result = f'[+] Detected {len(forms)} forms on {url}\n'

	for form in forms:
		form_details = get_form_details(form)

		for c in "\"'":
			data = {}

			for input in form_details['inputs']:
				if (input['type'] != 'submit' and input['type'] != 'password'):
					data[input['name']] = f"test{c}"
				
				if input['type'] == 'password':
					data[input['name']] = 'test'

			url = urljoin(url, form_details['action'])

			if form_details['method'] == 'post':
				response = s.post(url, data=data)
			
			if form_details['method'] == 'get':
				response = s.get(url, params=data)

			if is_vulnerable(response):
				result += f"""\n[+] SQL Injection vulnerability detected, link: {url}\n"""
				log.info(f'[+] SQL Injection vulnerability detected, link: {url}')
				return result
			
	result += f"""\n[-] No SQL Injection vulnerability detected, link: {url}\n"""
	log.info(f'[-] No SQL Injection vulnerability detected, link: {url}')
	return result


def submit_form(form_details, url, value):
	"""
	Submits a form given in `form_details`
	Returns the HTTP Response after form submission
	"""

	target_url = urljoin(url, form_details["action"])
	inputs = form_details["inputs"]
	data = {}

	for input in inputs:
		if input["type"] == "text" or input["type"] == "search":
			input["value"] = value
		
		input_name = input.get("name")
		input_value = input.get("value")
		
		if input_name and input_value:
			data[input_name] = input_value

	log.info(f"[!] Submitting malicious payload to {target_url}")
	log.info(f"[+] Data: {data}")
	print(f"[!] Submitting malicious payload to {target_url}")
	print(f"[+] Data: {data}")

	if form_details["method"] == "post":
		return requests.post(target_url, data=data)
	
	else:
		return requests.get(target_url, params=data)


def xss(url):
	"""
	Given a `url`, it prints all XSS vulnerable forms and 
	returns True if any is vulnerable, False otherwise
	"""

	found = False

	log.info(f'[!] Trying XSS on {url}')

	result = None
	forms = get_forms(url)
	log.info(f"[+] Detected {len(forms)} forms on {url}")
	print(f"[+] Detected {len(forms)} forms on {url}")
	
	js_script = "<script>alert('alert')</script>"
	
	for form in forms:
		form_details = get_form_details(form)
		content = submit_form(form_details, url, js_script).content.decode()

		if js_script in content:
			found = True
			

			result = f"[+] XSS Detected on {url}"
			result += f"\n[*] Form details : "
			result += '\n' + str(form_details)
	
	if found:
		log.info(f"[+] XSS Detected on {url}")
	
	else:
		log.info(f"[-] No XSS Vulnerability detected in {url}")
		result = f'[-] No XSS Vulnerability detected in {url}'

	return result

def web_crawl(url):
    threads = 10
    base_domain = urlparse(url).netloc
    visited_urls = set()
    urls_to_visit = Queue()
    urls_to_visit.put(url)

    def scrape_links():
        while not urls_to_visit.empty():
            url = urls_to_visit.get()
            try:
                response = requests.get(url)
                soup = bs(response.text, "html.parser")
                links = soup.find_all("a", href=True)
                for link in links:
                    href = link.get("href")
                    if href and href not in visited_urls and href != '#':
                        print(f"[+] Found new link: {href}")
                        visited_urls.add(href)
                        absolute_url = urljoin(url, href)
                        if urlparse(absolute_url).netloc == base_domain:
                            urls_to_visit.put(absolute_url)
                        
                            # print(f"[+] Found new link: {absolute_url}")
            except requests.ConnectionError:
                pass


    def create_threads():
        for _ in range(threads):
            t = threading.Thread(target=scrape_links)
            t.start()
    
    create_threads()