import aiohttp
from bs4 import BeautifulSoup as bs
from urllib.parse import urljoin, urlparse


async def fetch(url, session):
	async with session.get(url) as response:
		try:
			return await response.text(encoding='utf-8')
		except UnicodeDecodeError:
			return await response.text(errors='replace')

async def web_crawl(base_url, recursive):
	if recursive:
		if not base_url.startswith('http://') and not base_url.startswith('https://'):
			base_url = 'https://' + base_url
		base_domain = urlparse(base_url).netloc
		visited_urls = set()
		urls_to_visit = [base_url]
		found_links = set()

		async with aiohttp.ClientSession() as session:
			async def scrape_links(url):
				nonlocal urls_to_visit
				try:
					html = await fetch(url, session)
					soup = bs(html, "html.parser")

					links = soup.find_all("a", href=True)
					for link in links:
						href = link.get("href")
						if href and href not in visited_urls and href != '#':
							print(f"[+] Found new link: {href}")
							visited_urls.add(href)
							absolute_url = urljoin(base_url, href)
							if urlparse(absolute_url).netloc == base_domain:
								urls_to_visit.append(absolute_url)
							
							found_links.add(f"{absolute_url}")

					form_links = soup.find_all('form', action=True)
					for link in form_links:
						link = link.get('action')
						if link and link not in visited_urls and link != '#':
							print(f"[+] Found new form link: {link}")
							visited_urls.add(link)
							absolute_url = urljoin(base_url, link)
							if urlparse(absolute_url).netloc == base_domain:
								urls_to_visit.append(absolute_url)
							
							found_links.add(f"{absolute_url}")

					js_links = soup.find_all('script', src=True)
					for link in js_links:
						link = link.get('src')
						if link and link not in visited_urls and link != '#':
							print(f"[+] Found new script source link: {link}")
							visited_urls.add(link)
							absolute_url = urljoin(base_url, link)
							if urlparse(absolute_url).netloc == base_domain:
								urls_to_visit.append(absolute_url)
							
							found_links.add(f"{absolute_url}")

					img_links = soup.find_all('img', src=True)
					for link in img_links:
						link = link.get('src')
						if link and link not in visited_urls and link != '#':
							print(f"[+] Found new image path: {link}")
							visited_urls.add(link)
							absolute_url = urljoin(base_url, link)
							if urlparse(absolute_url).netloc == base_domain:
								urls_to_visit.append(absolute_url)
							
							found_links.add(f"{absolute_url}")

				except aiohttp.ClientConnectionError:
					pass

			async def process_urls():
				while urls_to_visit:
					url = urls_to_visit.pop(0)
					await scrape_links(url)

			await process_urls()
		
		return found_links
	
	else:
		if not base_url.startswith('http://') and not base_url.startswith('https://'):
			base_url = 'https://' + base_url
		base_domain = urlparse(base_url).netloc
		visited_urls = set()
		found_links = set()

		async with aiohttp.ClientSession() as session:
			async def scrape_links(url):
				try:
					html = await fetch(url, session)
					soup = bs(html, "html.parser")

					links = soup.find_all("a", href=True)
					for link in links:
						href = link.get("href")
						if href and href not in visited_urls and href != '#':
							print(f"[+] Found new link: {href}")
							visited_urls.add(href)
							absolute_url = urljoin(base_url, href)							
							found_links.add(f"{absolute_url}")

					form_links = soup.find_all('form', action=True)
					for link in form_links:
						link = link.get('action')
						if link and link not in visited_urls and link != '#':
							print(f"[+] Found new form link: {link}")
							visited_urls.add(link)
							absolute_url = urljoin(base_url, link)
							found_links.add(f"{absolute_url}")

					js_links = soup.find_all('script', src=True)
					for link in js_links:
						link = link.get('src')
						if link and link not in visited_urls and link != '#':
							print(f"[+] Found new script source link: {link}")
							visited_urls.add(link)
							absolute_url = urljoin(base_url, link)
							found_links.add(f"{absolute_url}")

					img_links = soup.find_all('img', src=True)
					for link in img_links:
						link = link.get('src')
						if link and link not in visited_urls and link != '#':
							print(f"[+] Found new image path: {link}")
							visited_urls.add(link)
							absolute_url = urljoin(base_url, link)
							found_links.add(f"{absolute_url}")

				except aiohttp.ClientConnectionError:
					pass


			await scrape_links(base_url)
		
		return found_links
